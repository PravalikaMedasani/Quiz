{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hOJmHhs0LnY",
        "outputId": "d91801e9-cbe4-4815-8c3e-baa7ab8bd6fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "erHKucWR1ilO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9XdY1O5L2WPP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "def Create_model(learning_rate=0.001,batch_size=32,optimizer=\"Adam\"):\n",
        "  model=Sequential([Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "  model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "yZx4ZVnW2c9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94GlrfSE602M",
        "outputId": "f9a9dcf9-7663-4850-f32b-f06741ce91e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 64s 33ms/step - loss: 0.1287 - accuracy: 0.9612 - val_loss: 0.0510 - val_accuracy: 0.9843 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0436 - accuracy: 0.9870 - val_loss: 0.0284 - val_accuracy: 0.9906 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0297 - accuracy: 0.9906 - val_loss: 0.0375 - val_accuracy: 0.9880 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.0385 - val_accuracy: 0.9883 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 55s 30ms/step - loss: 0.0164 - accuracy: 0.9946 - val_loss: 0.0325 - val_accuracy: 0.9904 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0223 - val_accuracy: 0.9929 - lr: 2.0000e-04\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0228 - val_accuracy: 0.9930 - lr: 2.0000e-04\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0274 - val_accuracy: 0.9929 - lr: 2.0000e-04\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 55s 30ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0289 - val_accuracy: 0.9930 - lr: 2.0000e-04\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 6.2657e-04 - accuracy: 0.9999 - val_loss: 0.0274 - val_accuracy: 0.9934 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: %.2f%%' % (accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bBEWciM8g6B",
        "outputId": "963dddaa-a71b-40e6-894f-af35e16e958d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0274 - accuracy: 0.9934\n",
            "Accuracy: 99.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Classification\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load IMDb dataset\n",
        "num_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Preprocess the text data\n",
        "max_len = 500\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "2Id9e9E-9G3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(learning_rate=0.001, optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Embedding(num_words, 32),\n",
        "        SimpleRNN(32),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "V1LBZG589i82"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters to tune\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'optimizer': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "# Create KerasClassifier for GridSearchCV\n",
        "model = KerasClassifier(build_fn=create_model, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Perform grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "h4ihlhAh9ruY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best parameters\n",
        "best_params = grid_result.best_params_\n",
        "learning_rate = best_params['learning_rate']\n",
        "optimizer = best_params['optimizer']\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model(learning_rate=learning_rate, optimizer=optimizer)\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test), callbacks=[reduce_lr])\n",
        "\n",
        "# Evaluate the model on test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "6yOLhG329yug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}